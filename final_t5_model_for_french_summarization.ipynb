{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "final t5 model for french summarization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/claudelkros/Fine-tuning-for-french-dataset/blob/main/final_t5_model_for_french_summarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJlKGSUOWp4r"
      },
      "source": [
        "# Fine Tuning Transformer for Summary Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7T7zoFXaWp4z"
      },
      "source": [
        "\n",
        "### Introduction\n",
        "\n",
        "In this tutorial we will be fine tuning a transformer model for **Summarization Task**. \n",
        "In this task a summary of a given article/document is generated when passed through a network. There are 2 types of summary generation mechanisms:\n",
        "\n",
        "1. ***Extractive Summary:*** the network calculates the most important sentences from the article and gets them together to provide the most meaningful information from the article.\n",
        "2. ***Abstractive Summary***: The network creates new sentences to encapsulate maximum gist of the article and generates that as output. The sentences in the summary may or may not be contained in the article. \n",
        "\n",
        "In this tutorial we will be generating ***Abstractive Summary***. \n",
        "\n",
        "#### Flow of the notebook\n",
        "\n",
        "* As with all the tutorials previously, this notebook also follows a easy to follow steps. Making the process of fine tuning and training a Transformers model a straight forward task.\n",
        "* However, unlike the other notebooks, in the tutorial, most of the sections have been created into functions, and they are called from the `main()` in the end of the notebook. \n",
        "* This is done to leverage the [Weights and Biases Service](https://www.wandb.com/) WandB in short.\n",
        "* It is a experiment tracking, parameter optimization and artifact management service. That can be very easily integrated to any of the Deep learning or Machine learning frameworks. \n",
        "\n",
        "The notebook will be divided into separate sections to provide a organized walk through for the process used. This process can be modified for individual use cases. The sections are:\n",
        "\n",
        "1. [Preparing Environment and Importing Libraries](#section01)\n",
        "2. [Preparing the Dataset for data processing: Class](#section02)\n",
        "3. [Fine Tuning the Model: Function](#section03)\n",
        "4. [Validating the Model Performance: Function](#section04)\n",
        "5. [Main Function](#section05)\n",
        "    * [Initializing WandB](#section501)\n",
        "    * [Importing and Pre-Processing the domain data](#section502)\n",
        "    * [Creation of Dataset and Dataloader](#section503)\n",
        "    * [Neural Network and Optimizer](#section504)\n",
        "    * [Training Model and Logging to WandB](#section505)\n",
        "    * [Validation and generation of Summary](#section506)\n",
        "6. [Examples of the Summary Generated from the model](#section06)\n",
        "\n",
        "\n",
        "#### Technical Details\n",
        "\n",
        "This script leverages on multiple tools designed by other teams. Details of the tools used below. Please ensure that these elements are present in your setup to successfully implement this script.\n",
        "\n",
        "- **Data**:\n",
        "\t- We are using the News Summary dataset available at [Kaggle](https://www.kaggle.com/sunnysai12345/news-summary)\n",
        "\t- This dataset is the collection created from Newspapers published in India, extracting, details that are listed below.  We are referring only to the first csv file from the data dump: `news_summary.csv`\n",
        "\t- There are`4514` rows of data.  Where each row has the following data-point:\n",
        "\t\t- **author** : Author of the article\n",
        "\t\t- **date** : Date the article was published\n",
        "\t\t- **headline**: Headline for the published article\n",
        "\t\t- **read_more** : URL for the article to follow online\n",
        "\t\t- **text**: This is the summary of the article\n",
        "\t\t- **ctext**: This is the complete article\n",
        "\n",
        "\n",
        "- **Language Model Used**: \n",
        "    - This notebook uses one of the most recent and novel transformers model ***T5***. [Research Paper](https://arxiv.org/abs/1910.10683)    \n",
        "    - ***T5*** in many ways is one of its kind transformers architecture that not only gives state of the art results in many NLP tasks, but also has a very radical approach to NLP tasks.\n",
        "    - **Text-2-Text** - According to the graphic taken from the T5 paper. All NLP tasks are converted to a **text-to-text** problem. Tasks such as translation, classification, summarization and question answering, all of them are treated as a text-to-text conversion problem, rather than seen as separate unique problem statements.\n",
        "    - **Unified approach for NLP Deep Learning** - Since the task is reflected purely in the text input and output, you can use the same model, objective, training procedure, and decoding process to ANY task. Above framework can be used for any task - show Q&A, summarization, etc. \n",
        "   - We will be taking inputs from the T5 paper to prepare our dataset prior to fine tuning and training.    \n",
        "   - [Documentation for python](https://huggingface.co/transformers/model_doc/t5.html)\n",
        "\n",
        "![**Each NLP problem as a “text-to-text” problem** - input: text, output: text](https://miro.medium.com/max/4006/1*D0J1gNQf8vrrUpKeyD8wPA.png) \n",
        "\t \n",
        "\n",
        "\n",
        "- Hardware Requirements: \n",
        "\t- Python 3.6 and above\n",
        "\t- Pytorch, Transformers and\n",
        "\t- All the stock Python ML Library\n",
        "\t- GPU enabled setup \n",
        "   \n",
        "\n",
        "- **Script Objective**:\n",
        "\t- The objective of this script is to fine tune ***T5 *** to be able to generate summary, that a close to or better than the actual summary  while ensuring the important information from the article is not lost.\n",
        "\n",
        "---\n",
        "NOTE: \n",
        "We are using the Weights and Biases Tool-set in  this tutorial. The different components will be explained as we go through the article."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXRkXroFWp41"
      },
      "source": [
        "<a id='section01'></a>\n",
        "### Preparing Environment and Importing Libraries\n",
        "\n",
        "At this step we will be installing the necessary libraries followed by importing the libraries and modules needed to run our script. \n",
        "We will be installing:\n",
        "* transformers\n",
        "* wandb\n",
        "\n",
        "Libraries imported are:\n",
        "* Pandas\n",
        "* Pytorch\n",
        "* Pytorch Utils for Dataset and Dataloader\n",
        "* Transformers\n",
        "* T5 Model and Tokenizer\n",
        "* wandb\n",
        "\n",
        "Followed by that we will preapre the device for CUDA execeution. This configuration is needed if you want to leverage on onboard GPU. First we will check the GPU avaiable to us, using the nvidia command followed by defining our device.\n",
        "\n",
        "Finally, we will be logging into the [wandb](https://www.wandb.com/) serice using the login command"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WD_vnyLXZQzD"
      },
      "source": [
        "!pip install transformers -q\n",
        "!pip install wandb -q\n",
        "\n",
        "# Code for TPU packages install\n",
        "# !curl -q https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
        "# !python pytorch-xla-env-setup.py --apt-packages libomp5 libopenblas-dev"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzM1_ykHaFur"
      },
      "source": [
        "# Importing stock libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# Importing the T5 modules from huggingface/transformers\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "# WandB – Import the wandb library\n",
        "import wandb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvPxXdKJguYB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ebcdf53-3aa9-49a3-a5bd-23558f637f4f"
      },
      "source": [
        "# Checking out the GPU we have access to. This is output is from the google colab version. \n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat Nov 21 00:32:44 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.38       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P8     9W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLxxwd1scQNv"
      },
      "source": [
        "# # Setting up the device for GPU usage\n",
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "\n",
        "# Preparing for TPU usage\n",
        "# import torch_xla\n",
        "# import torch_xla.core.xla_model as xm\n",
        "# device = xm.xla_device()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-ePh9dEKXMw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "202b6134-dd6f-49ec-8dfd-823e43f7deba"
      },
      "source": [
        "# Login to wandb to log the model run and all the parameters\n",
        "!wandb login"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mclaudelkros\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5g1e5JPQWp8-"
      },
      "source": [
        "<a id='section02'></a>\n",
        "### Preparing the Dataset for data processing: Class\n",
        "\n",
        "We will start with creation of Dataset class - This defines how the text is pre-processed before sending it to the neural network. This dataset will be used the the Dataloader method that will feed  the data in batches to the neural network for suitable training and processing. \n",
        "The Dataloader and Dataset will be used inside the `main()`.\n",
        "Dataset and Dataloader are constructs of the PyTorch library for defining and controlling the data pre-processing and its passage to neural network. For further reading into Dataset and Dataloader read the [docs at PyTorch](https://pytorch.org/docs/stable/data.html)\n",
        "\n",
        "#### *CustomDataset* Dataset Class\n",
        "- This class is defined to accept the Dataframe as input and generate tokenized output that is used by the **T5** model for training. \n",
        "- We are using the **T5** tokenizer to tokenize the data in the `text` and `ctext` column of the dataframe. \n",
        "- The tokenizer uses the ` batch_encode_plus` method to perform tokenization and generate the necessary outputs, namely: `source_id`, `source_mask` from the actual text and `target_id` and `target_mask` from the summary text.\n",
        "- To read further into the tokenizer, [refer to this document](https://huggingface.co/transformers/model_doc/t5.html#t5tokenizer)\n",
        "- The *CustomDataset* class is used to create 2 datasets, for training and for validation.\n",
        "- *Training Dataset* is used to fine tune the model: **80% of the original data**\n",
        "- *Validation Dataset* is used to evaluate the performance of the model. The model has not seen this data during training. \n",
        "\n",
        "#### Dataloader: Called inside the `main()`\n",
        "- Dataloader is used to for creating training and validation dataloader that load data to the neural network in a defined manner. This is needed because all the data from the dataset cannot be loaded to the memory at once, hence the amount of data loaded to the memory and then passed to the neural network needs to be controlled.\n",
        "- This control is achieved using the parameters such as `batch_size` and `max_len`.\n",
        "- Training and Validation dataloaders are used in the training and validation part of the flow respectively"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "932p8NhxeNw4"
      },
      "source": [
        "# Creating a custom dataset for reading the dataframe and loading it into the dataloader to pass it to the neural network at a later stage for finetuning the model and to prepare it for predictions\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "\n",
        "    def __init__(self, dataframe, tokenizer, source_len, summ_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.source_len = source_len\n",
        "        self.summ_len = summ_len\n",
        "        self.text = self.data.text\n",
        "        self.summary = self.data.summary\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        summary = str(self.summary[index])\n",
        "        summary = ' '.join(summary.split())\n",
        "\n",
        "        text = str(self.text[index])\n",
        "        text = ' '.join(text.split())\n",
        "\n",
        "        source = self.tokenizer.batch_encode_plus([text], max_length= self.source_len, pad_to_max_length=True,return_tensors='pt')\n",
        "        target = self.tokenizer.batch_encode_plus([summary], max_length= self.summ_len, pad_to_max_length=True,return_tensors='pt')\n",
        "\n",
        "        source_ids = source['input_ids'].squeeze()\n",
        "        source_mask = source['attention_mask'].squeeze()\n",
        "        target_ids = target['input_ids'].squeeze()\n",
        "        target_mask = target['attention_mask'].squeeze()\n",
        "\n",
        "        return {\n",
        "            'source_ids': source_ids.to(dtype=torch.long), \n",
        "            'source_mask': source_mask.to(dtype=torch.long), \n",
        "            'target_ids': target_ids.to(dtype=torch.long),\n",
        "            'target_ids_y': target_ids.to(dtype=torch.long)\n",
        "        }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4Mhj130Wp-r"
      },
      "source": [
        "<a id='section03'></a>\n",
        "### Fine Tuning the Model: Function\n",
        "\n",
        "Here we define a training function that trains the model on the training dataset created above, specified number of times (EPOCH), An epoch defines how many times the complete data will be passed through the network. \n",
        "\n",
        "This function is called in the `main()`\n",
        "\n",
        "Following events happen in this function to fine tune the neural network:\n",
        "- The epoch, tokenizer, model, device details, testing_ dataloader and optimizer are passed to the `train ()` when its called from the `main()`\n",
        "- The dataloader passes data to the model based on the batch size.\n",
        "- `language_model_labels` are calculated from the `target_ids` also, `source_id` and `attention_mask` are extracted.\n",
        "- The model outputs first element gives the loss for the forward pass. \n",
        "- Loss value is used to optimize the weights of the neurons in the network.\n",
        "- After every 10 steps the loss value is logged in the wandb service. This log is then used to generate graphs for analysis. Such as [these](https://app.wandb.ai/abhimishra-91/transformers_tutorials_summarization?workspace=user-abhimishra-91)\n",
        "- After every 500 steps the loss value is printed in the console."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wqqimfh6xn1k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aeea51c4-7452-41aa-e0de-d227c2fececb"
      },
      "source": [
        "%mkdir checkpoint best_model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘checkpoint’: File exists\n",
            "mkdir: cannot create directory ‘best_model’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XyXP4g7Zx04l"
      },
      "source": [
        "def save_ckp(state, is_best, checkpoint_path, best_model_path):\n",
        "    \"\"\"\n",
        "    state: checkpoint we want to save\n",
        "    is_best: is this the best checkpoint; min validation loss\n",
        "    checkpoint_path: path to save checkpoint\n",
        "    best_model_path: path to save best model\n",
        "    \"\"\"\n",
        "    f_path = checkpoint_path\n",
        "    # save checkpoint data to the path given, checkpoint_path\n",
        "    torch.save(state, f_path)\n",
        "    # if it is a best model, min validation loss\n",
        "    if is_best:\n",
        "        best_fpath = best_model_path\n",
        "        # copy that checkpoint file to best path given, best_model_path\n",
        "        shutil.copyfile(f_path, best_fpath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKwWItTzx8lz"
      },
      "source": [
        "def load_ckp(checkpoint_fpath, model, optimizer):\n",
        "    \"\"\"\n",
        "    checkpoint_path: path to save checkpoint\n",
        "    model: model that we want to load checkpoint parameters into       \n",
        "    optimizer: optimizer we defined in previous training\n",
        "    \"\"\"\n",
        "    # load check point\n",
        "    checkpoint = torch.load(checkpoint_fpath)\n",
        "    # initialize state_dict from checkpoint to model\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "    # initialize optimizer from checkpoint to optimizer\n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "    # initialize valid_loss_min from checkpoint to valid_loss_min\n",
        "    valid_loss_min = checkpoint['valid_loss_min']\n",
        "    # return model, optimizer, epoch value, min validation loss \n",
        "    return model, optimizer, checkpoint['epoch'], valid_loss_min.item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBb4Bj9_LEJ0"
      },
      "source": [
        "import dill"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SaPAR7TWmxoM"
      },
      "source": [
        "# Creating the training function. This will be called in the main function. It is run depending on the epoch value.\n",
        "# The model is put into train mode and then we wnumerate over the training loader and passed to the defined network \n",
        "\n",
        "def train(epoch, tokenizer, model, device, loader, optimizer):\n",
        "    model.train()\n",
        "    torch.save(model.state_dict(), 'model_simple.pt')\n",
        "    for _,data in enumerate(loader, 0):\n",
        "        y = data['target_ids'].to(device, dtype = torch.long)\n",
        "        y_ids = y[:, :-1].contiguous()\n",
        "        lm_labels = y[:, 1:].clone().detach()\n",
        "        lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
        "        ids = data['source_ids'].to(device, dtype = torch.long)\n",
        "        mask = data['source_mask'].to(device, dtype = torch.long)\n",
        "\n",
        "        outputs = model(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids, lm_labels=lm_labels)\n",
        "        loss = outputs[0]\n",
        "        \n",
        "        if _%10 == 0:\n",
        "            wandb.log({\"Training Loss\": loss.item()})\n",
        "\n",
        "        if _%500==0:\n",
        "            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # xm.optimizer_step(optimizer)\n",
        "        # xm.mark_step()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YclZIjrWp-7"
      },
      "source": [
        "<a id='section04'></a>\n",
        "### Validating the Model Performance: Function\n",
        "\n",
        "During the validation stage we pass the unseen data(Testing Dataset), trained model, tokenizer and device details to the function to perform the validation run. This step generates new summary for dataset that it has not seen during the training session. \n",
        "\n",
        "This function is called in the `main()`\n",
        "\n",
        "This unseen data is the 20% of `news_summary.csv` which was seperated during the Dataset creation stage. \n",
        "During the validation stage the weights of the model are not updated. We use the generate method for generating new text for the summary. \n",
        "\n",
        "It depends on the `Beam-Search coding` method developed for sequence generation for models with LM head. \n",
        "\n",
        "The generated text and originally summary are decoded from tokens to text and returned to the `main()`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9TNdHlQ0CLz"
      },
      "source": [
        "def validate(epoch, tokenizer, model, device, loader):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    actuals = []\n",
        "    with torch.no_grad():\n",
        "        for _, data in enumerate(loader, 0):\n",
        "            y = data['target_ids'].to(device, dtype = torch.long)\n",
        "            ids = data['source_ids'].to(device, dtype = torch.long)\n",
        "            mask = data['source_mask'].to(device, dtype = torch.long)\n",
        "\n",
        "            generated_ids = model.generate(\n",
        "                input_ids = ids,\n",
        "                attention_mask = mask, \n",
        "                max_length=150, \n",
        "                num_beams=2,\n",
        "                repetition_penalty=2.5, \n",
        "                length_penalty=1.0, \n",
        "                early_stopping=True\n",
        "                )\n",
        "            preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
        "            target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]\n",
        "            if _%100==0:\n",
        "                print(f'Completed {_}')\n",
        "\n",
        "            predictions.extend(preds)\n",
        "            actuals.extend(target)\n",
        "    return predictions, actuals"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1EA50-VWp_J"
      },
      "source": [
        "<a id='section05'></a>\n",
        "### Main Function\n",
        "\n",
        "The `main()` as the name suggests is the central location to execute all the functions/flows created above in the notebook. The following steps are executed in the `main()`:\n",
        "\n",
        "\n",
        "<a id='section501'></a>\n",
        "#### Initializing WandB \n",
        "\n",
        "* The `main()` begins with initializing WandB run under a specific project. This command initiates a new run for each execution of this command. \n",
        "\n",
        "* Before we proceed any further i will give a brief overview of the **[WandB Service](https://www.wandb.com/)**\n",
        "\n",
        "* This service has been created to track ML experiments, Optimize the experiments and save artifacts. It is designed to seamlessly integrate with all the Machine Learning and Deep Learning Frameworks. Each script can be organized into *Project* and each execution of the script will be registered as a *run* in the respective project.\n",
        "\n",
        "* The service can be configured to log several default metrics, such a network weights, hardware usage, gradients and weights of the network. \n",
        "\n",
        "* It can also be used to log user defined metrics, such a loss in the `train()`.\n",
        "\n",
        "* This particular tutorial is logged in the project: **[transformers_tutorials_summarization](https://app.wandb.ai/abhimishra-91/transformers_tutorials_summarization?workspace=user-abhimishra-91)**\n",
        "\n",
        "**One of the charts from the project**\n",
        "![](https://github.com/abhimishra91/transformers-tutorials/blob/master/meta/wandb.png?raw=1)\n",
        "\n",
        "* Visit the project page to see the details of different runs and what information is logged by the service. \n",
        "\n",
        "* Following the initialization of the WandB service we define configuration parameters that will be used across the tutorial such as `batch_size`, `epoch`, `learning_rate` etc.\n",
        "\n",
        "* These parameters are also passed to the WandB config. The config construct with all the parameters can be optimized using the Sweep service from WandB. Currently, that is outof scope of this tutorial. \n",
        "\n",
        "* Next we defining seed values so that the experiment and results can be reproduced.\n",
        "\n",
        "\n",
        "<a id='section502'></a>\n",
        "#### Importing and Pre-Processing the domain data\n",
        "\n",
        "We will be working with the data and preparing it for fine tuning purposes. \n",
        "*Assuming that the `news_summary.csv` is already downloaded in your `data` folder*\n",
        "\n",
        "* The file is imported as a dataframe and give it the headers as per the documentation.\n",
        "* Cleaning the file to remove the unwanted columns.\n",
        "* A new string is added to the main article column `summarize: ` prior to the actual article. This is done because **T5** had similar formatting for the summarization dataset. \n",
        "* The final Dataframe will be something like this:\n",
        "\n",
        "|text|ctext|\n",
        "|--|--|\n",
        "|summary-1|summarize: article 1|\n",
        "|summary-2|summarize: article 2|\n",
        "|summary-3|summarize: article 3|\n",
        "\n",
        "* Top 5 rows of the dataframe are printed on the console.\n",
        "\n",
        "<a id='section503'></a>\n",
        "#### Creation of Dataset and Dataloader\n",
        "\n",
        "* The updated dataframe is divided into 80-20 ratio for test and validation. \n",
        "* Both the data-frames are passed to the `CustomerDataset` class for tokenization of the new articles and their summaries.\n",
        "* The tokenization is done using the length parameters passed to the class.\n",
        "* Train and Validation parameters are defined and passed to the `pytorch Dataloader contstruct` to create `train` and `validation` data loaders.\n",
        "* These dataloaders will be passed to `train()` and `validate()` respectively for training and validation action.\n",
        "* The shape of datasets is printed in the console.\n",
        "\n",
        "\n",
        "<a id='section504'></a>\n",
        "#### Neural Network and Optimizer\n",
        "\n",
        "* In this stage we define the model and optimizer that will be used for training and to update the weights of the network. \n",
        "* We are using the `t5-base` transformer model for our project. You can read about the `T5 model` and its features above. \n",
        "* We use the `T5ForConditionalGeneration.from_pretrained(\"t5-base\")` commad to define our model. The `T5ForConditionalGeneration` adds a Language Model head to our `T5 model`. The Language Model head allows us to generate text based on the training of `T5 model`.\n",
        "* We are using the `Adam` optimizer for our project. This has been a standard for all our tutorials and is something that can be changed updated to see how different optimizer perform with different learning rates. \n",
        "* There is also a scope for doing more with Optimizer such a decay, momentum to dynamically update the Learning rate and other parameters. All those concepts have been kept out of scope for these tutorials. \n",
        "\n",
        "\n",
        "<a id='section505'></a>\n",
        "#### Training Model and Logging to WandB\n",
        "\n",
        "* Now we log all the metrics in WandB project that we have initialized above.\n",
        "* Followed by that we call the `train()` with all the necessary parameters.\n",
        "* Loss at every 500th step is printed on the console.\n",
        "* Loss at every 10th step is logged as Loss in the WandB service.\n",
        "\n",
        "\n",
        "<a id='section506'></a>\n",
        "#### Validation and generation of Summary\n",
        "\n",
        "* After the training is completed, the validation step is initiated.\n",
        "* As defined in the validation function, the model weights are not updated. We use the fine tuned model to generate new summaries based on the article text.\n",
        "* An output is printed on the console giving a count of how many steps are complete after every 100th step. \n",
        "* The original summary and generated summary are converted into a list and returned to the main function. \n",
        "* Both the lists are used to create the final dataframe with 2 columns **Generated Summary** and **Actual Summary**\n",
        "* The dataframe is saved as a csv file in the local drive.\n",
        "* A qualitative analysis can be done with the Dataframe. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6An36Jahbgy"
      },
      "source": [
        "import wandb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtGvjpT4huc1"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0MUPvlHsWAi"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBCB18FlxrUM"
      },
      "source": [
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import shutil\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-U_dHcyR2ulU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f5cc0f9-846a-4968-eb3e-2e9a51b7cfea"
      },
      "source": [
        "%mkdir models model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘models’: File exists\n",
            "mkdir: cannot create directory ‘model’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvnriqoUuUmm"
      },
      "source": [
        "import tensorflow as tf\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cq7ImkbqL5yV",
        "outputId": "250d0be1-14ac-43ca-9bf6-5a52b0603f29"
      },
      "source": [
        "!pip install datasets\n",
        "from datasets import load_dataset \n",
        "df = load_dataset('mlsum', 'fr')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.6/dist-packages (1.1.3)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from datasets) (0.3.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.6/dist-packages (from datasets) (2.0.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from datasets) (1.1.4)\n",
            "Requirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.6/dist-packages (from datasets) (2.0.0)\n",
            "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.6/dist-packages (from datasets) (4.41.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from datasets) (1.18.5)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from datasets) (0.7)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.6/dist-packages (from datasets) (0.70.10)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Reusing dataset mlsum (/root/.cache/huggingface/datasets/mlsum/fr/1.0.0/c5bff4e58cb961d638232afed081348215ca85c4687a04ae80f025d56ce18327)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8fULBvbS2lQ"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpQ2ewBLTYMX"
      },
      "source": [
        "text = df['train']['text']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqW49LeiToRm",
        "outputId": "d8689444-674d-4876-93c0-399b66201b9d"
      },
      "source": [
        "type(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abTiOKrkT8KG"
      },
      "source": [
        "dt = pd.DataFrame(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "id": "v-0bwFehUBH6",
        "outputId": "77f08420-00f4-40b0-bab1-fa119f9b7790"
      },
      "source": [
        "dt[0][0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Jean-Jacques Schuhl, Gilles Leroy, Christian Gailly, Yasmina Khadra, James Ellroy, Amos Oz, V. S. Naipaul… Si la rentrée de janvier s\\'annonce sous les meilleurs auspices, force est de constater que, avec 491 romans (contre 558 en 2009), la tendance à la baisse enregistrée à l\\'automne s\\'accentue. Principale victime de cette glaciation : la littérature étrangère, qui enregistre un recul de 21 % avec 167 romans, contre 211 l\\'an dernier. Soit son plus bas niveau depuis 2001. Peut-être doit-on voir là le contrecoup de l\\'augmentation des droits d\\'auteur et de traduction, mais aussi le fait que les organisateurs du Salon du livre ont choisi de célébrer les trente ans de la manifestation en invitant non pas un pays mais des écrivains français et étrangers. Si la littérature française, de son côté, marque un léger fléchissement avec 324 livres, contre 347 l\\'an passé, les premiers romans après un automne en demi-teinte repartent à la hausse avec 73 titres, contre 61 en 2009. Loin de l\\'effervescence de septembre et de la tension des prix, cette rentrée hivernale est placée sous le signe de la décrue. Elle n\\'en offre pas moins un programme de belle qualité. Ainsi chez Gallimard, où le retour de Jean-Jacques Schuhl avec Entrée des fantômes, dix ans après son Goncourt pour Ingrid Caven, est déjà annoncé comme l\\'un des événements de la rentrée. Tout comme le nouveau roman de Patrick Modiano, prévu en mars. Autres têtes d\\'affiche : Philippe Sollers avec Discours parfait, le troisième tome de ses articles, chroniques et entretiens ; Richard Millet, qui propose un récit et un roman où le Liban tient une grande place ; Camille Laurens, qui narre, avec Romance nerveuse, la rencontre d\\'une romancière et d\\'un paparazzi, ou encore Philippe Djian, dont le nouveau livre, Incidences, paraîtra en février. Du côté des filiales de Gallimard, notons la présence d\\'un autre Goncourt, Gilles Leroy, qui propose, au Mercure de France, Zola Jackson. Chez Verticales, Arnaud Cathrine poursuit son exploration du roman-choral avec Le Journal intime de Benjamin Lorca, tandis que Céline Minard, chez Denoël, propose avec Olimpia, un portrait sulfureux de la belle-sœur du pape Innocent X. Enfin, chez P.O.L., l\\'essai de Marie Darrieussecq, Rapport de police : accusation de plagiat et autres modes de surveillance de la fiction pourraient bien susciter quelques polémiques. Face à cette escouade, Grasset n\\'est pas en reste, qui publie le dernier roman de Jacques Chessex (décédé le 9 octobre), Le Dernier Crâne de M. de Sade, \"chesséien\" en diable. Mais aussi Troisième chronique du règne de Nicolas Ier, de Patrick Rambaud, l\\'émouvant roman Le Premier Amour de Véronique Olmi ou encore deux recueils de poèmes de Charles Dantzig. La maison sœur Fayard n\\'a pas à rougir d\\'un programme où l\\'on retrouvera le délicat Dominique Fabre, mais également Claire Castillon, Frédéric Vitoux, Cyrille Fleischman, Morgan Sportès et un nouveau Pierre Pelot des plus sanglant. En cette rentrée, la palme du recrutement en masse revient incontestablement à Actes Sud, qui affiche, au côté de Claude Pujade-Renaud, une myriade de transfuges. A commencer par Anne Weber (ex-Seuil), Emmelene Landon, venu de Léo Scheer, Denis Baldwin-Beneich (ex-Denoël), Emilie Frèche (Flammarion) ou encore Véronique Bizot,qui, après des nouvelles chez Stock, publie son premier roman. Le Seuil, à l\\'inverse, joue la carte des auteurs maison avec François Emmanuel, Olivier Rolin, Maryline Desbiolles, Patrick Grainville, Michèle Gazier ou encore Catherine Clément. Tout comme l\\'Olivier, qui poursuit son travail de mise en lumière de jeunes auteurs tels Valérie Zenatti, Jakuta Alikavazovic ou le drolatique Martin Page. Chez Stock, outre l\\'arrivée de Catherine Vigourt avec le très autobiographique Un jeune garçon, sont également très attendus le dernier roman de Luc Lang, Esprit chien, ainsi qu\\'en février un volumineux et très singulier roman de Marie Billetdoux, C\\'est encore moi qui vous écris (1968-2008), composé à partir de ses écrits intimes (lettres, bulletins scolaires, critiques de livres…). Toujours aussi éclectique, Flammarion offre un programme où se côtoient Diastème, Laurent Seksik, qui évoque de manière romanesque les derniers jours de Stephan Zweig, Brigitte Fontaine ou Andrée Chedid. Eternel oublié des prix d\\'automne, non sans s\\'en plaindre, Yasmina Khadra, chez Julliard, a donc choisi janvier pour publier L\\'Olympe des infortunes, une fable philosophique qui rompt avec ses précédents romans. Autre grand auteur algérien de cette rentrée, aux éditions de l\\'Aube, Maïssa Bey. Sous forme épistolaire, la romancière aborde, dans Puisque mon cœur est mort, le thème du pardon et de la loi de réconciliation nationale. Ce bref tour d\\'horizon ne pourrait s\\'achever sans évoquer les Editions de Minuit, qui proposent, comme en septembre, un duo de choix avec Eric Chevillard et Christian Gailly. Christine Rousseau'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cl_V8GpTesR"
      },
      "source": [
        "summary = df['train']['summary']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0qbx6vHSSR9"
      },
      "source": [
        "ds = pd.DataFrame(summary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ZFwbRz3UURqu",
        "outputId": "a9036e01-e039-4c9a-d3b0-d393449667e6"
      },
      "source": [
        "ds[0][0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Jean-Jacques Schuhl, Gilles Leroy, Christian Gailly, Yasmina Khadra, James Ellroy, Amos Oz, V. S. Naipaul… La rentrée de janvier s'annonce sous les meilleurs auspices.\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDsMU8EZVJsf"
      },
      "source": [
        "text_array = np.array(dt)\n",
        "summary_array = np.array(ds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCfkrtYFVW8W"
      },
      "source": [
        "column_text = ['text']\n",
        "column_summary = ['summary']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVBVg4_OVhNp"
      },
      "source": [
        "df_text = pd.DataFrame(data = text_array,columns = column_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSMe0O77Vjq2"
      },
      "source": [
        "df_summary = pd.DataFrame(data = summary_array,columns = column_summary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jj90HwSVtXu"
      },
      "source": [
        " frames = [df_text, df_summary]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5VS6lO5VxMF"
      },
      "source": [
        "results = pd.concat(frames, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "FcXwvl_XVy2M",
        "outputId": "29c0603a-f574-49ae-f2bd-14f053e299f6"
      },
      "source": [
        "results"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>summary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Jean-Jacques Schuhl, Gilles Leroy, Christian G...</td>\n",
              "      <td>Jean-Jacques Schuhl, Gilles Leroy, Christian G...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Une semaine après l'attaque terroriste manquée...</td>\n",
              "      <td>Cette demande intervient une semaine après l'a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Un juge américain a rejeté, jeudi 31 décembre,...</td>\n",
              "      <td>Un juge américain a rejeté jeudi les accusatio...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Un attentat a fait au moins 93 morts et plusie...</td>\n",
              "      <td>Un kamikaze a fait exploser sa voiture piégée ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Cinq personnes sont mortes, et treize autres o...</td>\n",
              "      <td>Cinq personnes sont mortes, et treize autres o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>392897</th>\n",
              "      <td>Seules les personnes employées par des particu...</td>\n",
              "      <td>Dès le mois de janvier, l’impôt sera désormais...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>392898</th>\n",
              "      <td>Carlos Ghosn à Paris, le 6 octobre 2017. MICHE...</td>\n",
              "      <td>L’ex-président du constructeur japonais Nissan...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>392899</th>\n",
              "      <td>Lors d’une manifestation anti-Brexit aux abord...</td>\n",
              "      <td>Le départ du Royaume-Uni de l’Union européenne...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>392900</th>\n",
              "      <td>La chancelière allemande Angela Merkel, peu ap...</td>\n",
              "      <td>Il appartient à Berlin de « tenir bon, argumen...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>392901</th>\n",
              "      <td>Depuis Noël, ils sont une centaine de migrants...</td>\n",
              "      <td>Il consiste notamment en un accroissement du n...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>392902 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                     text                                            summary\n",
              "0       Jean-Jacques Schuhl, Gilles Leroy, Christian G...  Jean-Jacques Schuhl, Gilles Leroy, Christian G...\n",
              "1       Une semaine après l'attaque terroriste manquée...  Cette demande intervient une semaine après l'a...\n",
              "2       Un juge américain a rejeté, jeudi 31 décembre,...  Un juge américain a rejeté jeudi les accusatio...\n",
              "3       Un attentat a fait au moins 93 morts et plusie...  Un kamikaze a fait exploser sa voiture piégée ...\n",
              "4       Cinq personnes sont mortes, et treize autres o...  Cinq personnes sont mortes, et treize autres o...\n",
              "...                                                   ...                                                ...\n",
              "392897  Seules les personnes employées par des particu...  Dès le mois de janvier, l’impôt sera désormais...\n",
              "392898  Carlos Ghosn à Paris, le 6 octobre 2017. MICHE...  L’ex-président du constructeur japonais Nissan...\n",
              "392899  Lors d’une manifestation anti-Brexit aux abord...  Le départ du Royaume-Uni de l’Union européenne...\n",
              "392900  La chancelière allemande Angela Merkel, peu ap...  Il appartient à Berlin de « tenir bon, argumen...\n",
              "392901  Depuis Noël, ils sont une centaine de migrants...  Il consiste notamment en un accroissement du n...\n",
              "\n",
              "[392902 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnScw8hwV4zD"
      },
      "source": [
        "results.to_csv('corpus.csv', encoding='utf-8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DOG7H0KuMFEQ",
        "outputId": "f7e0c38d-5c60-4584-ae6f-39741126db0c"
      },
      "source": [
        "# The full `train` split and the full `test` split as two distinct datasets.\n",
        "train_ds, test_ds = load_dataset('mlsum', 'fr', split=['train', 'test'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reusing dataset mlsum (/root/.cache/huggingface/datasets/mlsum/fr/1.0.0/c5bff4e58cb961d638232afed081348215ca85c4687a04ae80f025d56ce18327)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vurQ6iCMQie"
      },
      "source": [
        "train_ds['text']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtNs9ytpCow2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d2ff763e-c8b4-48f3-db93-8e30125aceec"
      },
      "source": [
        "def main():\n",
        "    # WandB – Initialize a new run\n",
        "    wandb.init(project=\"transformers_tutorials_summarization\")\n",
        "\n",
        "    # WandB – Config is a variable that holds and saves hyperparameters and inputs\n",
        "    # Defining some key variables that will be used later on in the training  \n",
        "    config = wandb.config          # Initialize config\n",
        "    config.TRAIN_BATCH_SIZE = 2    # input batch size for training (default: 64)\n",
        "    config.VALID_BATCH_SIZE = 2    # input batch size for testing (default: 1000)\n",
        "    config.TRAIN_EPOCHS = 2        # number of epochs to train (default: 10)\n",
        "    config.VAL_EPOCHS = 1 \n",
        "    config.LEARNING_RATE = 1e-4    # learning rate (default: 0.01)\n",
        "    config.SEED = 42               # random seed (default: 42)\n",
        "    config.MAX_LEN = 512\n",
        "    config.SUMMARY_LEN = 150 \n",
        "\n",
        "    # Set random seeds and deterministic pytorch for reproducibility\n",
        "    torch.manual_seed(config.SEED) # pytorch random seed\n",
        "    np.random.seed(config.SEED) # numpy random seed\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "    # tokenzier for encoding the text\n",
        "    tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
        "    \n",
        "\n",
        "    # Importing and Pre-Processing the domain data\n",
        "    # Selecting the needed columns only. \n",
        "    # Adding the summarzie text in front of the text. This is to format the dataset similar to how T5 model was trained for summarization task. \n",
        "    df = pd.read_csv('corpus.csv',encoding='utf-8')\n",
        "    df = df[['text','summary']]\n",
        "    df.text = 'summary: ' + df.text\n",
        "    print(df.head())\n",
        "\n",
        "    \n",
        "    # Creation of Dataset and Dataloader\n",
        "    # Defining the train size. So 80% of the data will be used for training and the rest will be used for validation. \n",
        "    train_size = 0.8\n",
        "    train_dataset=df.sample(frac=train_size,random_state = config.SEED)\n",
        "    val_dataset=df.drop(train_dataset.index).reset_index(drop=True)\n",
        "    train_dataset = train_dataset.reset_index(drop=True)\n",
        "\n",
        "    print(\"FULL Dataset: {}\".format(df.shape))\n",
        "    print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
        "    print(\"TEST Dataset: {}\".format(val_dataset.shape))\n",
        "\n",
        "\n",
        "    # Creating the Training and Validation dataset for further creation of Dataloader\n",
        "    training_set = CustomDataset(train_dataset, tokenizer, config.MAX_LEN, config.SUMMARY_LEN)\n",
        "    val_set = CustomDataset(val_dataset, tokenizer, config.MAX_LEN, config.SUMMARY_LEN)\n",
        "\n",
        "    # Defining the parameters for creation of dataloaders\n",
        "    train_params = {\n",
        "        'batch_size': config.TRAIN_BATCH_SIZE,\n",
        "        'shuffle': True,\n",
        "        'num_workers': 0\n",
        "        }\n",
        "\n",
        "    val_params = {\n",
        "        'batch_size': config.VALID_BATCH_SIZE,\n",
        "        'shuffle': False,\n",
        "        'num_workers': 0\n",
        "        }\n",
        "\n",
        "    # Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.\n",
        "    training_loader = DataLoader(training_set, **train_params)\n",
        "    val_loader = DataLoader(val_set, **val_params)\n",
        "\n",
        "\n",
        "    \n",
        "    # Defining the model. We are using t5-base model and added a Language model layer on top for generation of Summary. \n",
        "    # Further this model is sent to device (GPU/TPU) for using the hardware.\n",
        "    model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
        "    model = model.to(device)\n",
        "    \n",
        "    # Defining the optimizer that will be used to tune the weights of the network in the training session. \n",
        "    optimizer = torch.optim.Adam(params =  model.parameters(), lr=config.LEARNING_RATE)\n",
        "\n",
        "    # Log metrics with wandb\n",
        "    wandb.watch(model, log=\"all\")\n",
        "    # Training loop\n",
        "    print('Initiating Fine-Tuning for the model on our dataset')\n",
        "\n",
        "    for epoch in range(config.TRAIN_EPOCHS):\n",
        "        train(epoch, tokenizer, model, device, training_loader, optimizer)\n",
        "        \n",
        "    # Validation loop and saving the resulting file with predictions and acutals in a dataframe.\n",
        "    # Saving the dataframe as predictions.csv\n",
        "    print('Now generating summaries on our fine tuned model for the validation dataset and saving it in a dataframe')\n",
        "    for epoch in range(config.VAL_EPOCHS):\n",
        "      if torch.cuda.is_available():\n",
        "        predictions, actuals = validate(epoch, tokenizer, model, device, val_loader)\n",
        "        final_df = pd.DataFrame({'Generated Text':predictions,'Actual Text':actuals})\n",
        "        #final_df = pd.DataFrame({'Generated Text':to_summarize,'Actual Text':to_summarize})\n",
        "        final_df.to_csv('./models/predictions.csv')\n",
        "        print('Output Files generated for review')\n",
        "    \n",
        "    model.save_pretrained(\"model\")\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mclaudelkros\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.11<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">smooth-fire-75</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/claudelkros/transformers_tutorials_summarization\" target=\"_blank\">https://wandb.ai/claudelkros/transformers_tutorials_summarization</a><br/>\n",
              "                Run page: <a href=\"https://wandb.ai/claudelkros/transformers_tutorials_summarization/runs/1b56npwa\" target=\"_blank\">https://wandb.ai/claudelkros/transformers_tutorials_summarization/runs/1b56npwa</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20201121_003354-1b56npwa</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "                                                text                                            summary\n",
            "0  summary: Jean-Jacques Schuhl, Gilles Leroy, Ch...  Jean-Jacques Schuhl, Gilles Leroy, Christian G...\n",
            "1  summary: Une semaine après l'attaque terrorist...  Cette demande intervient une semaine après l'a...\n",
            "2  summary: Un juge américain a rejeté, jeudi 31 ...  Un juge américain a rejeté jeudi les accusatio...\n",
            "3  summary: Un attentat a fait au moins 93 morts ...  Un kamikaze a fait exploser sa voiture piégée ...\n",
            "4  summary: Cinq personnes sont mortes, et treize...  Cinq personnes sont mortes, et treize autres o...\n",
            "FULL Dataset: (392902, 2)\n",
            "TRAIN Dataset: (314322, 2)\n",
            "TEST Dataset: (78580, 2)\n",
            "Initiating Fine-Tuning for the model on our dataset\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/modeling_t5.py:1156: FutureWarning: The `lm_labels` argument is deprecated and will be removed in a future version, use `labels` instead.\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0, Loss:  5.971557140350342\n",
            "Epoch: 0, Loss:  1.138223648071289\n",
            "Epoch: 0, Loss:  1.7282518148422241\n",
            "Epoch: 0, Loss:  1.018807291984558\n",
            "Epoch: 0, Loss:  2.9447686672210693\n",
            "Epoch: 0, Loss:  2.448552131652832\n",
            "Epoch: 0, Loss:  2.4719560146331787\n",
            "Epoch: 0, Loss:  2.861159563064575\n",
            "Epoch: 0, Loss:  2.2647149562835693\n",
            "Epoch: 0, Loss:  0.4114845395088196\n",
            "Epoch: 0, Loss:  1.314096212387085\n",
            "Epoch: 0, Loss:  2.444927930831909\n",
            "Epoch: 0, Loss:  1.1524314880371094\n",
            "Epoch: 0, Loss:  1.794533610343933\n",
            "Epoch: 0, Loss:  2.7826409339904785\n",
            "Epoch: 0, Loss:  1.7409192323684692\n",
            "Epoch: 0, Loss:  1.7475813627243042\n",
            "Epoch: 0, Loss:  2.432558298110962\n",
            "Epoch: 0, Loss:  1.5646462440490723\n",
            "Epoch: 0, Loss:  1.9788837432861328\n",
            "Epoch: 0, Loss:  2.0601084232330322\n",
            "Epoch: 0, Loss:  0.9599894285202026\n",
            "Epoch: 0, Loss:  1.2457525730133057\n",
            "Epoch: 0, Loss:  2.4315831661224365\n",
            "Epoch: 0, Loss:  0.5035943388938904\n",
            "Epoch: 0, Loss:  2.127164125442505\n",
            "Epoch: 0, Loss:  0.9449363946914673\n",
            "Epoch: 0, Loss:  2.34767484664917\n",
            "Epoch: 0, Loss:  2.2674524784088135\n",
            "Epoch: 0, Loss:  0.5936092734336853\n",
            "Epoch: 0, Loss:  3.0458953380584717\n",
            "Epoch: 0, Loss:  1.7338271141052246\n",
            "Epoch: 0, Loss:  1.5294773578643799\n",
            "Epoch: 0, Loss:  0.7564024329185486\n",
            "Epoch: 0, Loss:  2.536212921142578\n",
            "Epoch: 0, Loss:  1.7036850452423096\n",
            "Epoch: 0, Loss:  1.3473479747772217\n",
            "Epoch: 0, Loss:  1.8183423280715942\n",
            "Epoch: 0, Loss:  1.5346852540969849\n",
            "Epoch: 0, Loss:  2.911656141281128\n",
            "Epoch: 0, Loss:  0.21060775220394135\n",
            "Epoch: 0, Loss:  1.5327727794647217\n",
            "Epoch: 0, Loss:  0.4556572139263153\n",
            "Epoch: 0, Loss:  1.0035736560821533\n",
            "Epoch: 0, Loss:  1.360153317451477\n",
            "Epoch: 0, Loss:  1.965111494064331\n",
            "Epoch: 0, Loss:  1.616581916809082\n",
            "Epoch: 0, Loss:  1.958254337310791\n",
            "Epoch: 0, Loss:  1.4269299507141113\n",
            "Epoch: 0, Loss:  0.7078076004981995\n",
            "Epoch: 0, Loss:  2.7979283332824707\n",
            "Epoch: 0, Loss:  1.533456563949585\n",
            "Epoch: 0, Loss:  2.0574471950531006\n",
            "Epoch: 0, Loss:  1.5816636085510254\n",
            "Epoch: 0, Loss:  2.332670211791992\n",
            "Epoch: 0, Loss:  1.0578967332839966\n",
            "Epoch: 0, Loss:  0.5064092874526978\n",
            "Epoch: 0, Loss:  1.716707468032837\n",
            "Epoch: 0, Loss:  2.218827247619629\n",
            "Epoch: 0, Loss:  1.6411617994308472\n",
            "Epoch: 0, Loss:  2.4972541332244873\n",
            "Epoch: 0, Loss:  0.8401085138320923\n",
            "Epoch: 0, Loss:  1.2141004800796509\n",
            "Epoch: 0, Loss:  2.3938310146331787\n",
            "Epoch: 0, Loss:  1.7998716831207275\n",
            "Epoch: 0, Loss:  1.514292597770691\n",
            "Epoch: 0, Loss:  1.6602238416671753\n",
            "Epoch: 0, Loss:  1.909167766571045\n",
            "Epoch: 0, Loss:  1.0643296241760254\n",
            "Epoch: 0, Loss:  2.2150766849517822\n",
            "Epoch: 0, Loss:  1.339779257774353\n",
            "Epoch: 0, Loss:  1.4352741241455078\n",
            "Epoch: 0, Loss:  2.3230626583099365\n",
            "Epoch: 0, Loss:  2.3533334732055664\n",
            "Epoch: 0, Loss:  1.0944483280181885\n",
            "Epoch: 0, Loss:  1.903432846069336\n",
            "Epoch: 0, Loss:  2.1604459285736084\n",
            "Epoch: 0, Loss:  1.146217942237854\n",
            "Epoch: 0, Loss:  0.8039474487304688\n",
            "Epoch: 0, Loss:  2.188695192337036\n",
            "Epoch: 0, Loss:  1.9103732109069824\n",
            "Epoch: 0, Loss:  0.6510223150253296\n",
            "Epoch: 0, Loss:  2.083622932434082\n",
            "Epoch: 0, Loss:  1.3752027750015259\n",
            "Epoch: 0, Loss:  0.6848601698875427\n",
            "Epoch: 0, Loss:  0.959227442741394\n",
            "Epoch: 0, Loss:  1.3770235776901245\n",
            "Epoch: 0, Loss:  1.7132315635681152\n",
            "Epoch: 0, Loss:  2.0853347778320312\n",
            "Epoch: 0, Loss:  2.2048277854919434\n",
            "Epoch: 0, Loss:  1.8049551248550415\n",
            "Epoch: 0, Loss:  1.5124231576919556\n",
            "Epoch: 0, Loss:  0.8064864277839661\n",
            "Epoch: 0, Loss:  0.1243586540222168\n",
            "Epoch: 0, Loss:  1.773605465888977\n",
            "Epoch: 0, Loss:  1.7310771942138672\n",
            "Epoch: 0, Loss:  0.7807148694992065\n",
            "Epoch: 0, Loss:  2.5888140201568604\n",
            "Epoch: 0, Loss:  2.125354290008545\n",
            "Epoch: 0, Loss:  0.47488492727279663\n",
            "Epoch: 0, Loss:  0.6917506456375122\n",
            "Epoch: 0, Loss:  1.1815696954727173\n",
            "Epoch: 0, Loss:  1.6281185150146484\n",
            "Epoch: 0, Loss:  3.0395126342773438\n",
            "Epoch: 0, Loss:  0.9242280125617981\n",
            "Epoch: 0, Loss:  0.8902451992034912\n",
            "Epoch: 0, Loss:  1.2585077285766602\n",
            "Epoch: 0, Loss:  1.6521762609481812\n",
            "Epoch: 0, Loss:  2.3432552814483643\n",
            "Epoch: 0, Loss:  1.3429646492004395\n",
            "Epoch: 0, Loss:  2.4625325202941895\n",
            "Epoch: 0, Loss:  1.9458681344985962\n",
            "Epoch: 0, Loss:  1.5242365598678589\n",
            "Epoch: 0, Loss:  1.1830101013183594\n",
            "Epoch: 0, Loss:  1.1528304815292358\n",
            "Epoch: 0, Loss:  1.2668335437774658\n",
            "Epoch: 0, Loss:  1.3930858373641968\n",
            "Epoch: 0, Loss:  1.814962387084961\n",
            "Epoch: 0, Loss:  0.8083322644233704\n",
            "Epoch: 0, Loss:  1.9738749265670776\n",
            "Epoch: 0, Loss:  0.8939527273178101\n",
            "Epoch: 0, Loss:  1.2366961240768433\n",
            "Epoch: 0, Loss:  2.42441987991333\n",
            "Epoch: 0, Loss:  0.7325215339660645\n",
            "Epoch: 0, Loss:  1.0120842456817627\n",
            "Epoch: 0, Loss:  1.0733059644699097\n",
            "Epoch: 0, Loss:  1.4145534038543701\n",
            "Epoch: 0, Loss:  2.0976202487945557\n",
            "Epoch: 0, Loss:  1.1981714963912964\n",
            "Epoch: 0, Loss:  0.3394544720649719\n",
            "Epoch: 0, Loss:  2.6453146934509277\n",
            "Epoch: 0, Loss:  2.155148506164551\n",
            "Epoch: 0, Loss:  1.6046135425567627\n",
            "Epoch: 0, Loss:  1.1099203824996948\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cd2ES7HmCoTv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec46ec39-b5a7-4af3-de68-ac71629dc073"
      },
      "source": [
        "!pip install git+https://github.com/tagucci/pythonrouge.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/tagucci/pythonrouge.git\n",
            "  Cloning https://github.com/tagucci/pythonrouge.git to /tmp/pip-req-build-qi0ssgbu\n",
            "  Running command git clone -q https://github.com/tagucci/pythonrouge.git /tmp/pip-req-build-qi0ssgbu\n",
            "Building wheels for collected packages: pythonrouge\n",
            "  Building wheel for pythonrouge (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pythonrouge: filename=pythonrouge-0.2-cp36-none-any.whl size=285402 sha256=c6930e3b2a0ace0ee2ee5487292c7044e0313a56a3f2e7745a428b11f146254c\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-_tu4o68p/wheels/fd/ff/be/6716935d513fa8656ab185cb0aa70aed382b72dda42bf09c95\n",
            "Successfully built pythonrouge\n",
            "Installing collected packages: pythonrouge\n",
            "Successfully installed pythonrouge-0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIC1FEhBCv8F"
      },
      "source": [
        "from pythonrouge.pythonrouge import Pythonrouge"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWr4_bK1C4OZ"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eu36LuLVC_tY"
      },
      "source": [
        "df = pd.read_csv('models/predictions.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQWRe08gHNvv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "d24ee7d1-470d-43e3-9eba-b3317a1153e1"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Generated Text</th>\n",
              "      <th>Actual Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Les réseaux de neurones sont généralement opti...</td>\n",
              "      <td>Si vous disposez d'ouvrages ou d'articles de r...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Cette année, les spécialistes des sciences cog...</td>\n",
              "      <td>modifier - modifier le code - modifier Wikidat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>cognitives étudient la cognition de divers poi...</td>\n",
              "      <td>Sur les autres projets Wikimedia :Le mot cogni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>Cette théorie de localisationniste des fonctio...</td>\n",
              "      <td>Pour améliorer la vérifiabilité de l'article, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>l'apparition de l'écriture distingue la Préhis...</td>\n",
              "      <td>L’écriture est un moyen de communication qui r...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  ...                                        Actual Text\n",
              "0           0  ...  Si vous disposez d'ouvrages ou d'articles de r...\n",
              "1           1  ...  modifier - modifier le code - modifier Wikidat...\n",
              "2           2  ...  Sur les autres projets Wikimedia :Le mot cogni...\n",
              "3           3  ...  Pour améliorer la vérifiabilité de l'article, ...\n",
              "4           4  ...  L’écriture est un moyen de communication qui r...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x69unOHxHYTQ"
      },
      "source": [
        "ref = []\n",
        "sum = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWJZtKLNHPOu"
      },
      "source": [
        " sum = df['Generated Text']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2gWXTukHAHw"
      },
      "source": [
        "ref = df['Actual Text']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYh9JdQRHhFd"
      },
      "source": [
        "ref = ref.to_numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HWBSsr8Ewzh"
      },
      "source": [
        "sum = sum.to_numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MP_98estIFsD"
      },
      "source": [
        "  rouge = Rouge() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hS45XgOtHttw"
      },
      "source": [
        "def rouge_metrics(x, y):\n",
        "  result = []\n",
        "  for ref, sum in zip(x, y):\n",
        "    result.append(rouge.get_scores(ref, sum))\n",
        "  return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkkdMbfsIqmS"
      },
      "source": [
        "val = rouge_metrics(ref, sum)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_Sw-9tyM77D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "930849e2-a2ae-40ec-b714-bc94102dc007"
      },
      "source": [
        "val"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[{'rouge-1': {'f': 0.19999999501250013,\n",
              "    'p': 0.21052631578947367,\n",
              "    'r': 0.19047619047619047},\n",
              "   'rouge-2': {'f': 0.0, 'p': 0.0, 'r': 0.0},\n",
              "   'rouge-l': {'f': 0.11650484939202586,\n",
              "    'p': 0.125,\n",
              "    'r': 0.10909090909090909}}],\n",
              " [{'rouge-1': {'f': 0.20952380589569167, 'p': 0.1375, 'r': 0.44},\n",
              "   'rouge-2': {'f': 0.038834947881987325,\n",
              "    'p': 0.02531645569620253,\n",
              "    'r': 0.08333333333333333},\n",
              "   'rouge-l': {'f': 0.09999999612812516,\n",
              "    'p': 0.06779661016949153,\n",
              "    'r': 0.19047619047619047}}],\n",
              " [{'rouge-1': {'f': 0.0, 'p': 0.0, 'r': 0.0},\n",
              "   'rouge-2': {'f': 0.0, 'p': 0.0, 'r': 0.0},\n",
              "   'rouge-l': {'f': 0.0, 'p': 0.0, 'r': 0.0}}],\n",
              " [{'rouge-1': {'f': 0.15909090626291325,\n",
              "    'p': 0.0958904109589041,\n",
              "    'r': 0.4666666666666667},\n",
              "   'rouge-2': {'f': 0.02325581122769096,\n",
              "    'p': 0.013888888888888888,\n",
              "    'r': 0.07142857142857142},\n",
              "   'rouge-l': {'f': 0.10810810521183355,\n",
              "    'p': 0.06557377049180328,\n",
              "    'r': 0.3076923076923077}}],\n",
              " [{'rouge-1': {'f': 0.19047618598009586,\n",
              "    'p': 0.14457831325301204,\n",
              "    'r': 0.27906976744186046},\n",
              "   'rouge-2': {'f': 0.048387092294485325,\n",
              "    'p': 0.036585365853658534,\n",
              "    'r': 0.07142857142857142},\n",
              "   'rouge-l': {'f': 0.10204081167846753,\n",
              "    'p': 0.08064516129032258,\n",
              "    'r': 0.1388888888888889}}]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-Q_2PizKvLU"
      },
      "source": [
        "def metrics(x):\n",
        "  metrics = []\n",
        "  f_score = x[0]['rouge-1']['f']\n",
        "  precision = x[0]['rouge-1']['p']\n",
        "  recall = x[0]['rouge-1']['r']\n",
        "  metrics.append(f_score)\n",
        "  metrics.append(precision)\n",
        "  metrics.append(recall)\n",
        "  return metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aT4xX0zCNATN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9e6fd75-ecae-4073-e58b-8cc4375f9c2e"
      },
      "source": [
        "metrics(val)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.19999999501250013, 0.21052631578947367, 0.19047619047619047]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spfcI8JXMJya"
      },
      "source": [
        "def loop(x):\n",
        "  items = []\n",
        "  for item in x:\n",
        "    items.append((metrics(item)))\n",
        "    return items"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpjwKrDvMamx"
      },
      "source": [
        "loop(val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i15jElEVKpeB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a25aaf92-7578-4b06-90c0-61e7a0508628"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.19999999501250013, 0.21052631578947367, 0.19047619047619047]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1su-ce7DyXE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1640e05-cbc6-4b35-94df-751469c9c4e5"
      },
      "source": [
        "!pip install rouge"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting rouge\n",
            "  Downloading https://files.pythonhosted.org/packages/43/cc/e18e33be20971ff73a056ebdb023476b5a545e744e3fc22acd8c758f1e0d/rouge-1.0.0-py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from rouge) (1.15.0)\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-1.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGa8apPVD3OZ"
      },
      "source": [
        "from rouge import Rouge \n",
        "\n",
        "hypothesis = \"the #### transcript is a written version of each day 's cnn student news program use this transcript to he    lp students with reading comprehension and vocabulary use the weekly newsquiz to test your knowledge of storie s you     saw on cnn student news\"\n",
        "\n",
        "reference = \"this page includes the show transcript use the transcript to help students with reading comprehension and     vocabulary at the bottom of the page , comment for a chance to be mentioned on cnn student news . you must be a teac    her or a student age # # or older to request a mention on the cnn student news roll call . the weekly newsquiz tests     students ' knowledge of even ts in the news\"\n",
        "\n",
        "rouge = Rouge()\n",
        "scores = rouge.get_scores(hypothesis, reference)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZ6psnGDEQPq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ff7e976-738a-4107-dcde-da448eaddb83"
      },
      "source": [
        "scores"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'rouge-1': {'f': 0.4786324739396596,\n",
              "   'p': 0.6363636363636364,\n",
              "   'r': 0.3835616438356164},\n",
              "  'rouge-2': {'f': 0.2608695605353498,\n",
              "   'p': 0.3488372093023256,\n",
              "   'r': 0.20833333333333334},\n",
              "  'rouge-l': {'f': 0.44705881864636676,\n",
              "   'p': 0.5277777777777778,\n",
              "   'r': 0.3877551020408163}}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dr3CPV_i7yDX"
      },
      "source": [
        "from transformers import T5Model, BertModel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MczKIRVs7qkx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "e814148c-b141-40ff-e286-4ef59cbc4825"
      },
      "source": [
        "model = BertModel.from_pretrained('best_model')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at best_model were not used when initializing BertModel: ['shared.weight', 'decoder.embed_tokens.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.final_layer_norm.weight', 'lm_head.weight', 'encoder.embed_tokens.weight', 'encoder.block.0.layer.0.SelfAttention.q.weight', 'encoder.block.0.layer.0.SelfAttention.k.weight', 'encoder.block.0.layer.0.SelfAttention.v.weight', 'encoder.block.0.layer.0.SelfAttention.o.weight', 'encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'encoder.block.0.layer.0.layer_norm.weight', 'encoder.block.0.layer.1.DenseReluDense.wi.weight', 'encoder.block.0.layer.1.DenseReluDense.wo.weight', 'encoder.block.0.layer.1.layer_norm.weight', 'encoder.block.1.layer.0.SelfAttention.q.weight', 'encoder.block.1.layer.0.SelfAttention.k.weight', 'encoder.block.1.layer.0.SelfAttention.v.weight', 'encoder.block.1.layer.0.SelfAttention.o.weight', 'encoder.block.1.layer.0.layer_norm.weight', 'encoder.block.1.layer.1.DenseReluDense.wi.weight', 'encoder.block.1.layer.1.DenseReluDense.wo.weight', 'encoder.block.1.layer.1.layer_norm.weight', 'encoder.block.2.layer.0.SelfAttention.q.weight', 'encoder.block.2.layer.0.SelfAttention.k.weight', 'encoder.block.2.layer.0.SelfAttention.v.weight', 'encoder.block.2.layer.0.SelfAttention.o.weight', 'encoder.block.2.layer.0.layer_norm.weight', 'encoder.block.2.layer.1.DenseReluDense.wi.weight', 'encoder.block.2.layer.1.DenseReluDense.wo.weight', 'encoder.block.2.layer.1.layer_norm.weight', 'encoder.block.3.layer.0.SelfAttention.q.weight', 'encoder.block.3.layer.0.SelfAttention.k.weight', 'encoder.block.3.layer.0.SelfAttention.v.weight', 'encoder.block.3.layer.0.SelfAttention.o.weight', 'encoder.block.3.layer.0.layer_norm.weight', 'encoder.block.3.layer.1.DenseReluDense.wi.weight', 'encoder.block.3.layer.1.DenseReluDense.wo.weight', 'encoder.block.3.layer.1.layer_norm.weight', 'encoder.block.4.layer.0.SelfAttention.q.weight', 'encoder.block.4.layer.0.SelfAttention.k.weight', 'encoder.block.4.layer.0.SelfAttention.v.weight', 'encoder.block.4.layer.0.SelfAttention.o.weight', 'encoder.block.4.layer.0.layer_norm.weight', 'encoder.block.4.layer.1.DenseReluDense.wi.weight', 'encoder.block.4.layer.1.DenseReluDense.wo.weight', 'encoder.block.4.layer.1.layer_norm.weight', 'encoder.block.5.layer.0.SelfAttention.q.weight', 'encoder.block.5.layer.0.SelfAttention.k.weight', 'encoder.block.5.layer.0.SelfAttention.v.weight', 'encoder.block.5.layer.0.SelfAttention.o.weight', 'encoder.block.5.layer.0.layer_norm.weight', 'encoder.block.5.layer.1.DenseReluDense.wi.weight', 'encoder.block.5.layer.1.DenseReluDense.wo.weight', 'encoder.block.5.layer.1.layer_norm.weight', 'encoder.block.6.layer.0.SelfAttention.q.weight', 'encoder.block.6.layer.0.SelfAttention.k.weight', 'encoder.block.6.layer.0.SelfAttention.v.weight', 'encoder.block.6.layer.0.SelfAttention.o.weight', 'encoder.block.6.layer.0.layer_norm.weight', 'encoder.block.6.layer.1.DenseReluDense.wi.weight', 'encoder.block.6.layer.1.DenseReluDense.wo.weight', 'encoder.block.6.layer.1.layer_norm.weight', 'encoder.block.7.layer.0.SelfAttention.q.weight', 'encoder.block.7.layer.0.SelfAttention.k.weight', 'encoder.block.7.layer.0.SelfAttention.v.weight', 'encoder.block.7.layer.0.SelfAttention.o.weight', 'encoder.block.7.layer.0.layer_norm.weight', 'encoder.block.7.layer.1.DenseReluDense.wi.weight', 'encoder.block.7.layer.1.DenseReluDense.wo.weight', 'encoder.block.7.layer.1.layer_norm.weight', 'encoder.block.8.layer.0.SelfAttention.q.weight', 'encoder.block.8.layer.0.SelfAttention.k.weight', 'encoder.block.8.layer.0.SelfAttention.v.weight', 'encoder.block.8.layer.0.SelfAttention.o.weight', 'encoder.block.8.layer.0.layer_norm.weight', 'encoder.block.8.layer.1.DenseReluDense.wi.weight', 'encoder.block.8.layer.1.DenseReluDense.wo.weight', 'encoder.block.8.layer.1.layer_norm.weight', 'encoder.block.9.layer.0.SelfAttention.q.weight', 'encoder.block.9.layer.0.SelfAttention.k.weight', 'encoder.block.9.layer.0.SelfAttention.v.weight', 'encoder.block.9.layer.0.SelfAttention.o.weight', 'encoder.block.9.layer.0.layer_norm.weight', 'encoder.block.9.layer.1.DenseReluDense.wi.weight', 'encoder.block.9.layer.1.DenseReluDense.wo.weight', 'encoder.block.9.layer.1.layer_norm.weight', 'encoder.block.10.layer.0.SelfAttention.q.weight', 'encoder.block.10.layer.0.SelfAttention.k.weight', 'encoder.block.10.layer.0.SelfAttention.v.weight', 'encoder.block.10.layer.0.SelfAttention.o.weight', 'encoder.block.10.layer.0.layer_norm.weight', 'encoder.block.10.layer.1.DenseReluDense.wi.weight', 'encoder.block.10.layer.1.DenseReluDense.wo.weight', 'encoder.block.10.layer.1.layer_norm.weight', 'encoder.block.11.layer.0.SelfAttention.q.weight', 'encoder.block.11.layer.0.SelfAttention.k.weight', 'encoder.block.11.layer.0.SelfAttention.v.weight', 'encoder.block.11.layer.0.SelfAttention.o.weight', 'encoder.block.11.layer.0.layer_norm.weight', 'encoder.block.11.layer.1.DenseReluDense.wi.weight', 'encoder.block.11.layer.1.DenseReluDense.wo.weight', 'encoder.block.11.layer.1.layer_norm.weight', 'encoder.final_layer_norm.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertModel were not initialized from the model checkpoint at best_model and are newly initialized: ['embeddings.word_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'pooler.dense.weight', 'pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7pK8mSjNHtD"
      },
      "source": [
        "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvycdGn6NLnO"
      },
      "source": [
        "from IPython.display import display, Markdown\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RY11vDANhE-j"
      },
      "source": [
        "from transformers import AutoModel, T5Model, T5ForConditionalGeneration\n",
        "model = T5ForConditionalGeneration.from_pretrained('model')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03lHuP2OqHt4"
      },
      "source": [
        "to_summarize = \"\"\"\n",
        "Wikipédia est un projet d’encyclopédie collective en ligne, universelle, multilingue et fonctionnant sur le principe du wiki. Ce projet vise à offrir un contenu librement réutilisable, objectif et vérifiable, que chacun peut modifier et améliorer.\n",
        "Wikipédia est définie par des principes fondateurs. Son contenu est sous licence Creative Commons BY-SA. Il peut être copié et réutilisé sous la même licence, sous réserve d'en respecter les conditions. Wikipédia fournit tous ses contenus gratuitement, \n",
        "sans publicité, et sans recourir à l'exploitation des données personnelles de ses utilisateurs.\n",
        "Les rédacteurs des articles de Wikipédia sont bénévoles. Ils coordonnent leurs efforts au sein d'une communauté collaborative, sans dirigeant.\n",
        "\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGYrx4l7Noye"
      },
      "source": [
        "test = \"\"\"\n",
        "Apprentissage automatique,\"Selon les informations disponibles durant la phase d'apprentissage, l'apprentissage est qualifié de différentes manières. Si les données sont étiquetées (c'est-à-dire que la réponse à la tâche est connue pour ces données), il s'agit d'un apprentissage supervisé. On parle de classification ou de classement[3] si les étiquettes sont discrètes, ou de régression si elles sont continues. Si le modèle est appris de manière incrémentale en fonction d'une récompense reçue par le programme pour chacune des actions entreprises, on parle d'apprentissage par renforcement. Dans le cas le plus général, sans étiquette, on cherche à déterminer la structure sous-jacente des données (qui peuvent être une densité de probabilité) et il s'agit alors d'apprentissage non supervisé. L'apprentissage automatique peut être appliqué à différents types de données, tels des graphes, des arbres, des courbes, ou plus simplement des vecteurs de caractéristiques, qui peuvent être continues ou discrètes.\n",
        " Depuis l'antiquité, le sujet des machines pensantes préoccupe les esprits. Ce concept est la base de pensées pour ce qui deviendra ensuite l'intelligence artificielle, ainsi qu'une de ses sous-branches : l'apprentissage automatique.\n",
        " La concrétisation de cette idée est principalement due à Alan Turing (mathématicien et cryptologue britannique) et à son concept de la « machine universelle » en 1936[4], qui est à la base des ordinateurs d'aujourd'hui. Il continuera à poser les bases de l'apprentissage automatique, avec son article sur « L'ordinateur et l'intelligence » en 1950[5], dans lequel il développe, entre autres, le test de Turing.\n",
        " En 1943, le neurophysiologiste Warren McCulloch et le mathématicien Walter Pitts publient un article décrivant le fonctionnement de neurones en les représentant à l'aide de circuits électriques. Cette représentation sera la base théorique des réseaux neuronaux[6].\n",
        " Arthur Samuel, informaticien américain pionnier dans le secteur de l'intelligence artificielle, est le premier à faire usage de l'expression machine learning (en français, « apprentissage automatique ») en 1959 à la suite de la création de son programme pour IBM en 1952. Le programme jouait au Jeu de Dames et s'améliorait en jouant. À terme, il parvint à battre le 4e meilleur joueur des États-Unis[7],[8].\n",
        " Une avancée majeure dans le secteur de l'intelligence machine est le succès de l'ordinateur développé par IBM, Deep Blue, qui est le premier à vaincre le champion mondial d'échecs Garry Kasparov en 1997. Le projet Deep Blue en inspirera nombre d'autres dans le cadre de l'intelligence artificielle, particulièrement un autre grand défi : IBM Watson, l'ordinateur dont le but est de gagner au jeu Jeopardy![9]. Ce but est atteint en 2011, quand Watson gagne à Jeopardy! en répondant aux questions par traitement de langage naturel[10].\n",
        " Durant les années suivantes, les applications de l'apprentissage automatique médiatisées se succèdent bien plus rapidement qu'auparavant.\n",
        "\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uknBTFqaKv7I",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116
        },
        "outputId": "65109c20-3ebc-45e5-e7d8-6bd728b60d47"
      },
      "source": [
        "#collapse-show\n",
        "#tokenizer = BartTokenizer.from_pretrained('bart-large-cnn')\n",
        "from transformers import T5Tokenizer\n",
        "#tokenizer = T5Tokenizer.from_pretrained(\"best_model/model\")\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
        "article_input_ids = tokenizer.batch_encode_plus([test], return_tensors='pt', max_length=1024)['input_ids']\n",
        "summary_ids = model.generate(article_input_ids,\n",
        "                             num_beams=4,\n",
        "                             length_penalty=2.0,\n",
        "                             max_length=145,\n",
        "                             min_len=56,\n",
        "                             no_repeat_ngram_size=3)\n",
        "\n",
        "summary_txt = tokenizer.decode(summary_ids.squeeze(), skip_special_tokens=True)\n",
        "display(Markdown('> **Summary: **'+summary_txt))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "> **Summary: **l'apprentissage automatique, c'est-à-dire la classification ou la régression des données, qui s'agit d'un apprentissage non supervisé[3]. Le projet Deep Blue est le premier dans le domaine de la machine à apprendre à partir des données établies en 1952[6].",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBlvAwq28tt-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f4b1be97-d49d-447a-f280-54744c2a94a6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7ZK9gPh9R3H"
      },
      "source": [
        "cp -r /content/model/ '/content/drive/My Drive/model/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5yMG5nDgesZ"
      },
      "source": [
        "cp -r '/content/drive/My Drive/model/'  '/content/best_model/'"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}